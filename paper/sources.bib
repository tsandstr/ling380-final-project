@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@inproceedings{hale2001probabilistic,
  title={A probabilistic Earley parser as a psycholinguistic model},
  author={Hale, John},
  booktitle={Second meeting of the north American chapter of the association for computational linguistics},
  year={2001}
}

@article{levy2008expectation,
  title={Expectation-based syntactic comprehension},
  author={Levy, Roger},
  journal={Cognition},
  volume={106},
  number={3},
  pages={1126--1177},
  year={2008},
  publisher={Elsevier}
}

@article{smith2013effect,
  title={The effect of word predictability on reading time is logarithmic},
  author={Smith, Nathaniel J and Levy, Roger},
  journal={Cognition},
  volume={128},
  number={3},
  pages={302--319},
  year={2013},
  publisher={Elsevier}
}

@inproceedings{wilcox-etal-2018-rnn,
    title = "What do {RNN} Language Models Learn about Filler{--}Gap Dependencies?",
    author = "Wilcox, Ethan  and
      Levy, Roger  and
      Morita, Takashi  and
      Futrell, Richard",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5423",
    doi = "10.18653/v1/W18-5423",
    pages = "211--221",
    abstract = "RNN language models have achieved state-of-the-art perplexity results and have proven useful in a suite of NLP tasks, but it is as yet unclear what syntactic generalizations they learn. Here we investigate whether state-of-the-art RNN language models represent long-distance \textbf{filler{--}gap dependencies} and constraints on them. Examining RNN behavior on experimentally controlled sentences designed to expose filler{--}gap dependencies, we show that RNNs can represent the relationship in multiple syntactic positions and over large spans of text. Furthermore, we show that RNNs learn a subset of the known restrictions on filler{--}gap dependencies, known as \textbf{island constraints}: RNNs show evidence for wh-islands, adjunct islands, and complex NP islands. These studies demonstrates that state-of-the-art RNN models are able to learn and generalize about empty syntactic positions.",
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@misc{gulordava2018colorless,
      title={Colorless green recurrent networks dream hierarchically}, 
      author={Kristina Gulordava and Piotr Bojanowski and Edouard Grave and Tal Linzen and Marco Baroni},
      year={2018},
      eprint={1803.11138},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{jozefowicz2016exploring,
      title={Exploring the Limits of Language Modeling}, 
      author={Rafal Jozefowicz and Oriol Vinyals and Mike Schuster and Noam Shazeer and Yonghui Wu},
      year={2016},
      eprint={1602.02410},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{ladusaw1980notion,
  title={On the notion affective in the analysis of negative-polarity items},
  author={Ladusaw, W},
  journal={Formal semantics: The essential readings},
  pages={457--470},
  year={1980}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{lakretz2019emergence,
    title = "The emergence of number and syntax units in {LSTM} language models",
    author = "Lakretz, Yair  and
      Kruszewski, German  and
      Desbordes, Theo  and
      Hupkes, Dieuwke  and
      Dehaene, Stanislas  and
      Baroni, Marco",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1002",
    doi = "10.18653/v1/N19-1002",
    pages = "11--20",
    abstract = "Recent work has shown that LSTMs trained on a generic language modeling objective capture syntax-sensitive generalizations such as long-distance number agreement. We have however no mechanistic understanding of how they accomplish this remarkable feat. Some have conjectured it depends on heuristics that do not truly take hierarchical structure into account. We present here a detailed study of the inner mechanics of number tracking in LSTMs at the single neuron level. We discover that long-distance number information is largely managed by two {``}number units{''}. Importantly, the behaviour of these units is partially controlled by other units independently shown to track syntactic structure. We conclude that LSTMs are, to some extent, implementing genuinely syntactic processing mechanisms, paving the way to a more general understanding of grammatical encoding in LSTMs.",
}

@article{xiang2009illusory,
  title={Illusory licensing effects across dependency types: ERP evidence},
  author={Xiang, Ming and Dillon, Brian and Phillips, Colin},
  journal={Brain and Language},
  volume={108},
  number={1},
  pages={40--55},
  year={2009},
  publisher={Elsevier}
}

@article{xiang2013dependency,
  title={Dependency-dependent interference: NPI interference, agreement attraction, and global pragmatic inferences},
  author={Xiang, Ming and Grove, Julian and Giannakidou, Anastasia},
  journal={Frontiers in psychology},
  volume={4},
  pages={708},
  year={2013},
  publisher={Frontiers}
}

@article{parker2016negative,
  title={Negative polarity illusions and the format of hierarchical encodings in memory},
  author={Parker, Dan and Phillips, Colin},
  journal={Cognition},
  volume={157},
  pages={321--339},
  year={2016},
  publisher={Elsevier}
}

@misc{jumelet2018language,
      title={Do Language Models Understand Anything? On the Ability of LSTMs to Understand Negative Polarity Items}, 
      author={Jaap Jumelet and Dieuwke Hupkes},
      year={2018},
      eprint={1808.10627},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{jumelet2021language,
      title={Language Models Use Monotonicity to Assess NPI Licensing}, 
      author={Jaap Jumelet and Milica DeniÄ‡ and Jakub Szymanik and Dieuwke Hupkes and Shane Steinert-Threlkeld},
      year={2021},
      eprint={2105.13818},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{clark2019does,
  title={What does bert look at? an analysis of bert's attention},
  author={Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D},
  journal={arXiv preprint arXiv:1906.04341},
  year={2019}
}

@inproceedings{jawahar2019does,
  title={What does BERT learn about the structure of language?},
  author={Jawahar, Ganesh and Sagot, Beno{\^\i}t and Seddah, Djam{\'e}},
  booktitle={ACL 2019-57th Annual Meeting of the Association for Computational Linguistics},
  year={2019}
}
