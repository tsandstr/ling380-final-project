% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt, round]{article}

% Remove the "review" option to generate the final version.
\usepackage{acl}
\usepackage{gb4e}
\noautomath
% Standard package includes
\usepackage{times}
\usepackage{latexsym}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subfig}
\graphicspath{{./img/}}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\usepackage{natbib}
\bibliographystyle{plainnat}

\title{Do LSTMs Understand the Licensing of Negative Polarity Items?}

\author{Theo Sandstrom \\
  \texttt{theo.sandstrom@yale.edu} \\
  Trey Skidmore \\
  \texttt{trey.skidmore@yale.edu} \\
  Prastik Mohanraj \\
  \texttt{prastik.mohanraj@yale.edu}}

\begin{document}
\maketitle

\begin{abstract}
While LSTMs have proven themselves as powerful tools for language modeling tasks, the extent to which their generalizations are based on underlying syntactic structure remains an open area of inquiry. We focus our investigation on the relationship between negative polarity items and their licensors in English. We probe the model's representation of this relationship by computing the network's surprisal upon encountering NPIs in licensed and unlicensed contexts. We show that...
\end{abstract}

\section{Introduction}

The development of Recurrent Neural Networks (RNN) has been a recent hallmark of advancements in Natural Language Processing (NLP). The inherent ability for RNNs to learn and internalize a number of linguistic capabilities is evidenced by a large corpus of research in the utilization of RNNs for NLP tasks such as machine translation, language modeling, and syntactic parsing \cite{wilcox-etal-2018-rnn}. However, it is still largely unknown how such networks are able to deduce linguistic information, in that there is comparatively less literature present regarding if networks' success is due to opportunistic surface-pattern-based heuristics or if networks are able to genuinely extract and interpret structural properties, syntactic or semantic, of language from their training data \cite{lakretz2019emergence}.

One subset of RNNs, namely Long Short-Term Memory RNNs (LSTMs), has grown in utility in said extraction of structural properties of language. Wilcox et al. \shortcite{wilcox-etal-2018-rnn} determined utilized LSTM models to explore the syntactic relationship of the filler-gap dependency, the relationship between a filler word (in English, a wh- complementizer like "what" or "when") and a gap word (in English, an empty syntactic position uniquely licensed by a filler word). Lakretz et al. \shortcite{lakretz2019emergence} similarly utilized LSTM models for exploring numerical agreement between verbs and their arguments. 

There is, though, little research investigating the feature of licensing of negative polarity items in language. Negative polarity items are a class of words that are only permitted to appear in certain “negative” contexts. For example, in English, negative polarity items include words like "ever," "any," and "even."
\begin{enumerate}
    \item He never gave up.
    \item *He \textit{ever} gave up.
    \item Nobody ever gave up.
\end{enumerate}
Here, the word "ever" is not permitted to occur in a sentence except when licensed by a word such as "nobody" that provides the surrounding "negative" context. Notably, the relationship between the licensor and the negative polarity item is based on the hierarchical structure of a sentence, wherein the licensor must c-command the negative polarity item (i.e. the negative polarity item must be a sister of the licensor or a direct descendant of a sister node of the licensor).

If a language model, after being trained on a sentence data set implementing the usage of negative polarity licensing, is able to predict the distribution of negative polarity items and the contexts in which they are licensed with significant accuracy, this would be evidence that the model is able to learn this feature of syntax, in particular that it is able to detect a relationship between a licensor and a negative polarity item.

\section{Methods}
\subsection{Language Models}

For our investigation, we trained a LSTM model on the WikiText-2 dataset \cite{merity2016pointer}, which consists of over 2 million tokens from 600 Wikipedia articles, with a vocabulary size of 33,279. Our LSTM has an embedding layer, assigning 200-dimensional embeddings to each word in the vocabulary, and two recurrent layers, each with 650 hidden units. During training, we use dropout regularization as described in Srivastava et. al. \shortcite{srivastava2014dropout}, with a dropout rate set at 0.2. Optimization was performed using gradient descent, with a base learning rate of 0.2, which we annealed whenever performance did not improve on the development set.

\subsection{Surprisal}

We assess the ability of an LSTM to predict the distribution of negative polarity items by using the surprisal metric that, as used in Wilcox et. al. \shortcite{wilcox-etal-2018-rnn}. The metric is defined to be the log inverse probability:
\[ S(x_i) = -\log_2 P(x_i \, | \, h_{i-1}), \]
where $x_i$ is the current word, $h_{i-1}$ is the LSTM's hidden state before $x$ and the probability is given by the softmax activation. The surprisal is measured in bits because we take a base two logarithm.

A high surprisal metric means that, in its context, a word was predicted with \textit{low} probability, and therefore was unexpected by the model. A direct correlation has been shown between language model surprisal metrics and  human sentence processing difficulty \cite{hale2001probabilistic,levy2008expectation,smith2013effect}. In order to probe the model’s understanding of negative polarity licensing, we construct a synthetic data set which contained both grammatical examples, in which negative polarity items are correctly licensed, and ungrammatical examples, in which the licensor is either not present or is not in a c-command relationship with the licensee.

\subsection{Experimental Design}

\section{Representation of Negative Polarity Items}

Negative polarity items have a variety of properties. Most obviously, they should only be present in negative contexts. They are also immune to intervening material. One negative context can license multiple negative polarity items. Lastly, the negative item must be in the correct syntactic position to license a negative polarity item. In this section we demonstrate that LSTMs have learned these four properties negative polarity items.

\subsection{Licensing of Negative Polarity Items}

As discussed in the introduction, negative polarity items are only grammatically correct in negative contexts. The following examples demonstrate this fact because only the sentences with negative contexts (a,c) are grammatically permissible:
\begin{exe}
\ex\label{ex:license-1}
\begin{xlist}
\ex Bill is not here yet.
\ex[*]{Bill is here yet.}
\end{xlist}
\ex\label{ex:license-2}
\begin{xlist}
\ex They hired somebody without any talent.
\ex[*]{They hired somebody with any talent.}
\end{xlist}
\end{exe}
In this subsection, we confirm the hypothesis that surprisal metrics for NPIs are reduced in the presence of an appropriate licensor. We created 20 test items containing negative polarity items. For each item, we constructed two variants: one with a licensor, and one without a licensor, as exemplified in Examples \ref{ex:license-1} and \ref{ex:license-2}.

Plotted in Figure \ref{fig:surprisal-basic-licensing} are graphs of surprisal on two generic examples; negative polarity items are assigned lower surprisal values by the model when they are found in grammatically licit environments. 

Overall, this experiment shows that our LSTM model has at least a basic understanding of when negative polarity items should be expected.

\begin{figure}
    \centering
    \subfloat[Grammatical]{{\includegraphics[width=.42\linewidth]{good1} }}
    \subfloat[Ungrammatical]{{\includegraphics[width=.42\linewidth]{bad1} }}
    \qquad
    \subfloat[Grammatical]{{\includegraphics[width=.42\linewidth]{good2} }}
    \subfloat[Ungrammatical]{{\includegraphics[width=.42\linewidth]{bad2} }}
    \caption{Plots of surprisal scores for words in the example sentences. Note that plots (a) and (c) are both contextually negative and plots (b) and (d) are positive. As expected, the surprisal score for the NPIs (\textit{yet} in plots (a) and (b) and \textit{any} in plots (c) and (d)) is much higher in positive contexts.}
    \label{fig:surprisal-basic-licensing}
\end{figure}

\subsection{Robustness of Negative Polarity Items to Intervening Material}
Syntactic dependencies are unchanged by the presence intervening material. In the following example, the presence of a negative polarity item (anymore) is due to syntactic licensing by negation (don't); modifying the object doesn't change the sentence structure, so it has no effect on NPI licensing:
\begin{exe}
\ex
\begin{xlist}
\ex We do not go to the supermarket anymore.
\ex We do not go to the supermarket that is owned by Mark anymore.
\ex We do not go down the block to the supermarket that is owned by Mark anymore. 
\end{xlist}
\end{exe}
We showed in the previous subsection that LSTMs have expectations for negative polarity items, so we attempt to determine whether those expectations remain over intervening material. For this subsection, we designed 10 sentences base sentences with NPI items. Each of the base sentences comes in three varieties: minimal intervening material (enough to make the sentence grammatical), 3-5 words of additional intervening material, and 6-10 additional words between the negative licensor and the polarity item. In all cases the intervening words modify the object of the base sentence.

\begin{figure*}
    \centering
    \subfloat[Base Sentence]{{\includegraphics[width=.32\textwidth]{short} }}
    \subfloat[5 Additional Words]{{\includegraphics[width=.32\textwidth]{medium} }}
    \subfloat[8 Additional Words]{{\includegraphics[width=.32\textwidth]{long} }}
    \caption{Plots of surprisal scores for words in the modified sentence. The surprisal score of the negative polarity item \textit{anymore} remains relatively unchanged by the amount of intervening material.}
\end{figure*}

An example of the results for this section are visualized in Figure 2. We found that in general, there was a weak positive correlation between the number of words in between the word that determined the negative context and the negative polarity item and the surprisal score. However as seen in the plots, the scores are relatively similar, so we can conclude that the model does not consider the NPIs as ungrammatical when the amount of intervening words is long.
\subsection{Licensing of Negative Polarity Items Between Separate C-Commands}
Just because one section of a sentence is in a negative context does not mean that it is appropriate to use a negative polarity item in another clause. In the following example, the dependent clause is fixed as negative and the independent clause (which contains the NPI) is varied, so only the first variation is correct:
\begin{exe}
\ex
\begin{xlist}
\ex Because he was not tired, he was not even sleeping
\ex [*]{Because he was not tired, he was even sleeping} 
\end{xlist}
\end{exe}
In this subsection we attempt to determine whether LSTMs are capable of understanding syntactic structure with regards to negative polarity items. In order to do this we designed 10 base sentences. Each sentence begins with a negative clause, and the grammatically correct version has a second negative clause and NPI wheras the incorrect version has a second postive clause and NPI. We are motivated to analyze this type of interaction because a model that is well adjusted to NPI should determine that the presence of negative context in a separate c-command does not influence whether it is actually grammatical.
\subsection{Ability to License Multiple Negative Polarity Items}
In most cases, a negative context can license multiple negative polarity items.
\begin{exe}
\ex
\begin{xlist}
\ex He didn't want to go with anybody anywhere
\ex [*]{He did want to go with anybody anywhere} 
\end{xlist}
\end{exe}

\section{Results}

\bibliography{sources}

\end{document}
