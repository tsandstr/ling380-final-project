@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@inproceedings{hale2001probabilistic,
  title={A probabilistic Earley parser as a psycholinguistic model},
  author={Hale, John},
  booktitle={Second meeting of the north American chapter of the association for computational linguistics},
  year={2001}
}

@article{levy2008expectation,
  title={Expectation-based syntactic comprehension},
  author={Levy, Roger},
  journal={Cognition},
  volume={106},
  number={3},
  pages={1126--1177},
  year={2008},
  publisher={Elsevier}
}

@article{smith2013effect,
  title={The effect of word predictability on reading time is logarithmic},
  author={Smith, Nathaniel J and Levy, Roger},
  journal={Cognition},
  volume={128},
  number={3},
  pages={302--319},
  year={2013},
  publisher={Elsevier}
}

@inproceedings{wilcox-etal-2018-rnn,
    title = "What do {RNN} Language Models Learn about Filler{--}Gap Dependencies?",
    author = "Wilcox, Ethan  and
      Levy, Roger  and
      Morita, Takashi  and
      Futrell, Richard",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5423",
    doi = "10.18653/v1/W18-5423",
    pages = "211--221",
    abstract = "RNN language models have achieved state-of-the-art perplexity results and have proven useful in a suite of NLP tasks, but it is as yet unclear what syntactic generalizations they learn. Here we investigate whether state-of-the-art RNN language models represent long-distance \textbf{filler{--}gap dependencies} and constraints on them. Examining RNN behavior on experimentally controlled sentences designed to expose filler{--}gap dependencies, we show that RNNs can represent the relationship in multiple syntactic positions and over large spans of text. Furthermore, we show that RNNs learn a subset of the known restrictions on filler{--}gap dependencies, known as \textbf{island constraints}: RNNs show evidence for wh-islands, adjunct islands, and complex NP islands. These studies demonstrates that state-of-the-art RNN models are able to learn and generalize about empty syntactic positions.",
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@misc{lakretz2019emergence,
      title={The emergence of number and syntax units in LSTM language models}, 
      author={Yair Lakretz and German Kruszewski and Theo Desbordes and Dieuwke Hupkes and Stanislas Dehaene and Marco Baroni},
      year={2019},
      eprint={1903.07435},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
